{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('precision', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1** (25 points)\n",
    "\n",
    "There is simulated data of 25,000 human heights and weights of 18 years old children at this URL\n",
    "`http://socr.ucla.edu/docs/resources/SOCR_Data/SOCR_Data_Dinov_020108_HeightsWeights.html`\n",
    "\n",
    "or this shortened version\n",
    "\n",
    "`'https://rb.gy/u3dvsb'`\n",
    "\n",
    "The original data has height in inches and weight in pounds. \n",
    "\n",
    "- Find the average height in cm, weight in kg, and BMI for each of the four categories defined below:\n",
    "\n",
    "```\n",
    "BMI             Category\n",
    "18 or less      Underweight\n",
    "18 – 22         Normal\n",
    "22 – 25         Overweight\n",
    "More than 25    Obese\n",
    "```\n",
    "\n",
    "- Do this in a reproducible way - that is, **all** processing of the data must be done in **code**\n",
    "- Use **method chaining** to generate the summary\n",
    "\n",
    "Your final table should look like this:\n",
    "\n",
    "| status      |   ht_in_cm |   wt_in_kg |     bmi |\n",
    "|:------------|-----------:|-----------:|--------:|\n",
    "| Underweight |    173.1 |    51.4 | 17.1 |\n",
    "| Normal      |    172.7 |    58.8 | 19.7 |\n",
    "| Overweight  |    170.6 |    65.9 | 22.6 |\n",
    "| Obese       |    166.1 |    70.0 | 25.4 |\n",
    "\n",
    "Notes\n",
    "\n",
    "- 1 inch = 2.54 cm\n",
    "- 1 pound = 0.453592 kg\n",
    "- BMI = weight in kg / (height in meters^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2** (25 points)\n",
    "\n",
    "Using the `EtLT` data pipeline pattern.\n",
    "\n",
    "- Using `requests`, download all people from the Star Wars REST API at https://swapi.dev/api and store the information about each person in a MongoDB database\n",
    "- Extract the data from MongoDB database\n",
    "- Flatten the nested JSON structure into a `pandas` DataFrame\n",
    "- Save to an SQLite3 database\n",
    "- Use SQL to transform the data in the SQLite3 dataase <font color=red>This question is not clear and was not considred in grading</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3** (50 points)\n",
    "\n",
    "The data set in `dm.csv` contains 11 columns as described below. The first 10 columns are features (X), and the last is the target (y). The features have been transformed such that the mean = zero, and the sum of squares = 0.\n",
    "\n",
    "```\n",
    "age     age in years\n",
    "sex\n",
    "bmi     body mass index\n",
    "bp      average blood pressure\n",
    "s1      tc, T-Cells (a type of white blood cells)\n",
    "s2      ldl, low-density lipoproteins\n",
    "s3      hdl, high-density lipoproteins\n",
    "s4      tch, thyroid stimulating hormone\n",
    "s5      ltg, lamotrigine\n",
    "s6      glu, blood sugar level\n",
    "target  measure of disease severity at 1 year\n",
    "```\n",
    "\n",
    "- Split the data into X_train, X_test, y_train, y_test\n",
    "- Plot the correlation matrix of X_train features as a heatmap using seaborn\n",
    "- Perform a PCA on X_train\n",
    "- Display a biplot of X_train projected on the first 2 principal components\n",
    "- Write a function that returns the number of components needed to explain p% of the variance in X_train, and show the result for p=90\n",
    "- Create a dummy regression model AND a proper regression model with sklearn's RandomForestRegressor to predict the target from the 10 features\n",
    "    - Perform hyperparameter optimization on at least 2 parameters of the Random Forest model using  GridSearch and create a tuned RandomForestRegressor model with the best parameters\n",
    "    - Plot the learning curve for the tuned Random Forest\n",
    "    - Evaluate the model performance on test data using R^2 and mean absolute error for both dummy and Random Forest models\n",
    "    - Plot feature importances for the Random Forest model using permutation importance and mean absolute Shapley values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

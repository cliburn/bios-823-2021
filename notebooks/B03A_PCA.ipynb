{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as la"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "np.set_printoptions(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA from scratch\n",
    "\n",
    "Principal Components Analysis (PCA) basically means to find and rank all the eigenvalues and eigenvectors of a covariance matrix. This is useful because high-dimensional data (with $p$ features) may have nearly all their variation in a small number of dimensions $k$, i.e. in the subspace spanned by the eigenvectors of the covariance matrix that have the $k$ largest eigenvalues. If we project the original data into this subspace, we can have a dimension reduction (from $p$ to $k$) with hopefully little loss of information.\n",
    "\n",
    "For zero-centered vectors,\n",
    "\n",
    "\\begin{align}\n",
    "\\text{Cov}(X, Y) &= \\frac{\\sum_{i=1}^n(X_i - \\bar{X})(Y_i - \\bar{Y})}{n-1} \\\\\n",
    "  &= \\frac{\\sum_{i=1}^nX_iY_i}{n-1} \\\\\n",
    "  &= \\frac{XY^T}{n-1}\n",
    "\\end{align}\n",
    "\n",
    "and so the covariance matrix for a data set X that has zero mean in each feature vector is just $XX^T/(n-1)$. \n",
    "\n",
    "In other words, we can also get the eigendecomposition of the covariance matrix from the positive semi-definite matrix $XX^T$.\n",
    "\n",
    "We will take advantage of this when we cover the SVD later in the course.\n",
    "\n",
    "Note: Here we are using a matrix of **row** vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "x1, x2, x3 = np.random.normal(0, 10, (3, n))\n",
    "x4 = x1 + np.random.normal(size=x1.shape)\n",
    "x5 = (x1 + x2)/2 + np.random.normal(size=x1.shape)\n",
    "x6 = (x1 + x2 + x3)/3 + np.random.normal(size=x1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For PCA calculations, each column is an observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.c_[x1, x2, x3, x4, x5, x6].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs[:, :10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Center each observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xc = xs - np.mean(xs, 1)[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xc[:, :10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Covariance\n",
    "\n",
    "Remember the formula for covariance\n",
    "\n",
    "$$\n",
    "\\text{Cov}(X, Y) = \\frac{\\sum_{i=1}^n(X_i - \\bar{X})(Y_i - \\bar{Y})}{n-1}\n",
    "$$\n",
    "\n",
    "where $\\text{Cov}(X, X)$ is the sample variance of $X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov = (xc @ xc.T)/(n-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.cov(xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eigendecomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e, v = la.eigh(cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.argsort(e)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = e[idx]\n",
    "v = v[:, idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explain the magnitude of the eigenvalues\n",
    "\n",
    "Note that $x4, x5, x6$ are linear combinations of $x1, x2, x3$ with some added noise, and hence the last 3 eigenvalues are small. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.stem(e)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The eigenvalues and eigenvectors give a factorization of the covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v @ np.diag(e) @ v.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geometry of PCA\n",
    "\n",
    "![Geometry off PCA](https://i.stack.imgur.com/AaF1w.jpg)\n",
    "\n",
    "### Algebra of PCA\n",
    "\n",
    "![Commuative diagram](figs/spectral.png)\n",
    "\n",
    "Note that $Q^T X$ results in a new data set that is uncorrelated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = np.zeros(2)\n",
    "s = np.array([[1, 0.8], [0.8, 1]])\n",
    "x = np.random.multivariate_normal(m, s, n).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate covariance matrix from centered observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xc = (x - x.mean(1)[:, np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov = (xc @ xc.T)/(n-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find eigendecoposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e, v = la.eigh(cov)\n",
    "idx = np.argsort(e)[::-1]\n",
    "e = e[idx]\n",
    "v = v[:, idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In original coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x[0], x[1], alpha=0.5)\n",
    "for e_, v_ in zip(e, v.T):\n",
    "    plt.plot([0, e_*v_[0]], [0, e_*v_[1]], 'r-', lw=2)\n",
    "plt.xlabel('x', fontsize=14)\n",
    "plt.ylabel('y', fontsize=14)\n",
    "plt.axis('square')\n",
    "plt.axis([-3,3,-3,3])\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After change of basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yc = v.T @ xc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(yc[0,:], yc[1,:], alpha=0.5)\n",
    "for e_, v_ in zip(e, np.eye(2)):\n",
    "    plt.plot([0, e_*v_[0]], [0, e_*v_[1]], 'r-', lw=2)\n",
    "plt.xlabel('PC1', fontsize=14)\n",
    "plt.ylabel('PC2', fontsize=14)\n",
    "plt.axis('square')\n",
    "plt.axis([-3,3,-3,3])\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note that the PCA from scikit-learn works with feature vectors, not observation vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = pca.fit_transform(x.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eigenvectors from PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eigenvalues from PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explained variance\n",
    "\n",
    "This is just a consequence of the invariance of the trace under change of basis. Since the original diagnonal entries in the covariance matrix are the variances of the featrues, the sum of the eigenvalues must also be the sum of the orignal varainces. In other words, the cumulateive proportion of the top $n$ eigenvaluee is the \"explained variance\" of the first $n$ principal components. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "e/e.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The principal components are identical to our home-brew version, up to a flip in direction of eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(z[:, 0], z[:, 1], alpha=0.5)\n",
    "for e_, v_ in zip(e, np.eye(2)):\n",
    "    plt.plot([0, e_*v_[0]], [0, e_*v_[1]], 'r-', lw=2)\n",
    "plt.xlabel('PC1', fontsize=14)\n",
    "plt.ylabel('PC2', fontsize=14)\n",
    "plt.axis('square')\n",
    "plt.axis([-3,3,-3,3])\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(121)\n",
    "plt.scatter(-z[:, 0], -z[:, 1], alpha=0.5)\n",
    "for e_, v_ in zip(e, np.eye(2)):\n",
    "    plt.plot([0, e_*v_[0]], [0, e_*v_[1]], 'r-', lw=2)\n",
    "plt.xlabel('PC1', fontsize=14)\n",
    "plt.ylabel('PC2', fontsize=14)\n",
    "plt.axis('square')\n",
    "plt.axis([-3,3,-3,3])\n",
    "plt.title('Scikit-learn PCA (flipped)')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.scatter(yc[0,:], yc[1,:], alpha=0.5)\n",
    "for e_, v_ in zip(e, np.eye(2)):\n",
    "    plt.plot([0, e_*v_[0]], [0, e_*v_[1]], 'r-', lw=2)\n",
    "plt.xlabel('PC1', fontsize=14)\n",
    "plt.ylabel('PC2', fontsize=14)\n",
    "plt.axis('square')\n",
    "plt.axis([-3,3,-3,3])\n",
    "plt.title('Homebrew PCA')\n",
    "plt.tight_layout()\n",
    "pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "frontiers-env",
   "language": "python",
   "name": "frontiers-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
